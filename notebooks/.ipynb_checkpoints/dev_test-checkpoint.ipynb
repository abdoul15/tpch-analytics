{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fb547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import psutil\n",
    "import os\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, datediff, round, sum, when, \n",
    "    current_timestamp, broadcast, expr\n",
    ")\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "# Création de la session Spark\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"FCT_Orders_Test\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "    .config(\"spark.default.parallelism\", \"10\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "def get_table_from_db(table_name: str, spark: SparkSession):\n",
    "    \"\"\"Fonction pour lire les données depuis PostgreSQL\"\"\"\n",
    "    host = \"upstream_data\"  # ou votre host PostgreSQL\n",
    "    port = \"5432\"      # ou votre port PostgreSQL\n",
    "    db = \"tpchdb\"      # ou votre nom de base de données\n",
    "    jdbc_url = f'jdbc:postgresql://{host}:{port}/{db}'\n",
    "    connection_properties = {\n",
    "        'user': 'tpchuser',          # votre username\n",
    "        'password': 'tpchpass',      # votre password\n",
    "        'driver': 'org.postgresql.Driver',\n",
    "    }\n",
    "    return spark.read.jdbc(\n",
    "        url=jdbc_url, table=table_name, properties=connection_properties\n",
    "    )\n",
    "\n",
    "def log_dataframe_info(df, step_name: str):\n",
    "    \"\"\"Utilitaire pour logger les infos d'un DataFrame\"\"\"\n",
    "    count = df.count()\n",
    "    partitions = df.rdd.getNumPartitions()\n",
    "    memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    print(f\"\"\"\n",
    "    ====== {step_name} ======\n",
    "    Count: {count}\n",
    "    Partitions: {partitions}\n",
    "    Memory Usage (MB): {memory:.2f}\n",
    "    Schema: {df.schema.simpleString()}\n",
    "    \"\"\")\n",
    "\n",
    "def process_batch(batch_df, orders_df, spark, batch_id):\n",
    "    \"\"\"Process a single batch of LineItem data\"\"\"\n",
    "    print(f\"Processing batch {batch_id}\")\n",
    "    \n",
    "    try:\n",
    "        # Jointure pour ce lot\n",
    "        joined_data = (\n",
    "            batch_df\n",
    "            .join(\n",
    "                orders_df,\n",
    "                batch_df.l_orderkey == orders_df.o_orderkey,\n",
    "                'inner'\n",
    "            )\n",
    "            .select([\n",
    "                col('o_orderkey').alias('order_key'),\n",
    "                col('l_linenumber').alias('line_number'),\n",
    "                col('o_custkey').alias('customer_key'),\n",
    "                col('l_partkey').alias('part_key'),\n",
    "                col('l_suppkey').alias('supplier_key'),\n",
    "                \n",
    "                # Dates\n",
    "                col('o_orderdate').alias('order_date'),\n",
    "                col('l_shipdate').alias('ship_date'),\n",
    "                col('l_commitdate').alias('commit_date'),\n",
    "                col('l_receiptdate').alias('receipt_date'),\n",
    "                \n",
    "                # Métriques\n",
    "                col('o_totalprice').alias('order_total_amount'),\n",
    "                col('o_orderpriority').alias('order_priority'),\n",
    "                col('o_orderstatus').alias('order_status'),\n",
    "                col('l_quantity').alias('quantity'),\n",
    "                col('l_extendedprice').alias('extended_price'),\n",
    "                col('l_discount').alias('discount_percentage'),\n",
    "                col('l_tax').alias('tax_percentage'),\n",
    "                \n",
    "                # Calculs optimisés\n",
    "                expr('ROUND(l_extendedprice * (1 - l_discount) * (1 + l_tax), 2)').alias('net_amount'),\n",
    "                expr('ROUND(l_extendedprice * l_discount, 2)').alias('discount_amount'),\n",
    "                expr('ROUND(l_extendedprice * (1 - l_discount) * l_tax, 2)').alias('tax_amount'),\n",
    "                \n",
    "                # Métriques de livraison\n",
    "                col('l_shipmode').alias('ship_mode'),\n",
    "                col('l_returnflag').alias('return_flag'),\n",
    "                col('l_linestatus').alias('line_status'),\n",
    "                expr('datediff(l_shipdate, o_orderdate)').alias('shipping_delay_days'),\n",
    "                expr('datediff(l_receiptdate, l_shipdate)').alias('delivery_delay_days'),\n",
    "                expr('l_receiptdate > l_commitdate').alias('is_late_delivery'),\n",
    "                \n",
    "                # Métadonnées\n",
    "                lit(current_timestamp()).alias('etl_inserted')\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        # Optimisation du résultat\n",
    "        result = (\n",
    "            joined_data\n",
    "            .repartition(20, 'order_key')\n",
    "            .persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "        )\n",
    "        \n",
    "        count = result.count()\n",
    "        print(f\"Batch {batch_id} produced {count} rows\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {batch_id}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Lecture des données sources\n",
    "orders_data = get_table_from_db('public.orders', spark)\n",
    "lineitem_data = get_table_from_db('public.lineitem', spark)\n",
    "\n",
    "# Log des volumes initiaux\n",
    "orders_count = orders_data.count()\n",
    "lineitem_count = lineitem_data.count()\n",
    "print(f\"Initial counts - Orders: {orders_count}, LineItem: {lineitem_count}\")\n",
    "\n",
    "# Optimisation des orders (plus petite table)\n",
    "orders_subset = (orders_data\n",
    "    .select([\n",
    "        'o_orderkey', 'o_custkey', 'o_orderdate',\n",
    "        'o_totalprice', 'o_orderpriority', 'o_orderstatus'\n",
    "    ])\n",
    "    .repartition(50, 'o_orderkey')\n",
    "    .persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    ")\n",
    "\n",
    "# Traitement par lots de LineItem\n",
    "batch_size = 1000000  # 1 million de lignes par lot\n",
    "num_batches = (lineitem_count + batch_size - 1) // batch_size\n",
    "print(f\"Processing LineItem in {num_batches} batches of {batch_size} rows\")\n",
    "\n",
    "# Création d'un DataFrame vide pour accumuler les résultats\n",
    "result_df = None\n",
    "\n",
    "for batch_id in range(num_batches):\n",
    "    print(f\"Processing batch {batch_id + 1}/{num_batches}\")\n",
    "    \n",
    "    # Sélection du lot\n",
    "    batch_df = (lineitem_data\n",
    "        .orderBy('l_orderkey')\n",
    "        .limit(batch_size)\n",
    "        .offset(batch_id * batch_size)\n",
    "    )\n",
    "\n",
    "    # Traitement du lot\n",
    "    batch_result = process_batch(\n",
    "        batch_df, \n",
    "        orders_subset, \n",
    "        spark, \n",
    "        batch_id\n",
    "    )\n",
    "\n",
    "    # Accumulation des résultats\n",
    "    if result_df is None:\n",
    "        result_df = batch_result\n",
    "    else:\n",
    "        result_df = result_df.unionByName(batch_result)\n",
    "\n",
    "    # Nettoyage explicite\n",
    "    batch_df.unpersist()\n",
    "    batch_result.unpersist()\n",
    "\n",
    "# Nettoyage final\n",
    "orders_subset.unpersist()\n",
    "\n",
    "# Afficher quelques résultats\n",
    "print(\"\\nFinal Results Sample:\")\n",
    "result_df.show(5)\n",
    "\n",
    "# Afficher les statistiques finales\n",
    "log_dataframe_info(result_df, \"Final Results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
